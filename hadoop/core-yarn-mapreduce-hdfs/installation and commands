Hadoop Installation
-------------------
--> sudo apt-update search java, then install an appropriate version of java which is jdk 8 in this case (need to get the right syntax)
	java -version
	sudo apt install openjdk-8-jre-headless
	java -version
	
--> sudo apt-get install rsync

--> download hadoop binaries 2.7.3 and extract them to /home/user/bigdata folder, this is the HADOOP_HOME directory

--> add the following parameters at the end of ~/.bashrc file
	export HADOOP_HOME=/home/user/bigdata/hadoop-2.7.3
	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-i386
	export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME
run the command source ~/.bashrc or ~./bashrc to implement the changes

--> Add export JAVA_HOME=JAVA_HOME=/usr/lib/jvm/java-8-openjdk-i386  in mapred_env.sh , hadoop_env.sh and yarn_env.sh files which are located in /home/user/bigdata/hadoop-2.7.3/etc/hadoop folder, by commenting existing JAVA_HOME settings

--> cd /home/user/bigdata/hadoop-2.7.3/share/hadoop/mapreduce
Run an example
hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount test.txt 123


Installing Hadoop in Pseudo Distributed mode
--------------------------------------------
--> Open the /etc/hosts file and add the entry for the hostname along with its IP.

--> ssh-keygen -t rsa
    Your identification has been saved in /home/user/.ssh/id_rsa.
    Your public key has been saved in /home/user/.ssh/id_rsa.pub.


--> cp id_rsa.pub authorized_keys
	Copy the file to authorized_keys file (more keys can be added into this single file later , while helps in password less logging into remote systems via ssh)

--> apt-cache policy openssh-server
	verify if the if the ssh server has been installed on the machine.

--> If not installed run the following commands to start the open ssh server
sudo apt-get remove --purge openssh-server
sudo apt-get install openssh-server
sudo service ssh status
sudo service ssh restart 

--> ssh <hostname> i.e., ssh ubuntu, when loggin into the host via ssh for the first time , an entry will be created in the known_hosts file.
	NOTE: make sure that you enter the options yes/no explicitly, do not assume that the default value is yes.
	
[--> if netstat is not installed installed it by running sudo apt install net_tools]
	
--> Open the core-site.xml file in $HADOOP_HOME/etc/hadoop and add following configuration. This tells the address of the namename server.
	<configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://ubuntu:9000</value>
      </property>
	</configuration>
	
--> Open hdfs-site.xml file in $HADOOP_HOME/etc/hadoop and add the following parameters 
	<configuration>
	  <property>
			<name>dfs.replication</name>
			<value>1</value>
	  </property>
	  <property>
			<name>dfs.namenode.name.dir</name>  <!--the default location is /tmp/hadoop-user -->
			<value>/home/user/bigdata/hdfsfilesystem/namenode</value>
	  </property>
	  </property>
			<name>dfs.datanode.data.dir</name>
			<value>/home/user/bigdata/hdfsfilesystem/datanode</value>
	  </property>
	</configuration>
	
--> Open mapred-site.xml file in $HADOOP_HOME/etc/hadoop and add the following parameters 
<configuration>
	 <property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	  </property>
	</configuration>

--> Open yarn_site.xml file in $HADOOP_HOME/etc/hadoop and add the following parameters
<configuration>
	<property>
	  <name>yarn.resourcemgr.hostname</name>
	  <value>ubuntu</value>
	</property>
	<property>
	  <name>yarn.nodemanager.aux-services</name>
	  <value>mapreduce_shuffle</value>
	</property>
</configuration>

--> hdfs namenode -format, run the command to format the filesystem for the first time , the defualt file system is in /tmp/hdfs...etc/hadoop

--> cd /home/user/bigdata/hadoop-2.7.3/sbin, this folder contains all scripts to start the processes.
	execute ./start-all.sh
	check out the log files into which the processes will write on startup , notice the console , during the starting of all the processes.
	
--> namenode browser in : http://192.168.80.129:50070
--> resource manager browser in : http://192.168.80.129:8088/cluster

--> jps 
           Command to see the list of java processes that are running. Typically we should below HDFS services:
	1556 : NameNode
	1636 : DataNode
	1740 : SecondaryNameNode
	       yarn services:
	1893 : ResourceManager
	1975 : NodeManager ( this is not showing up)
--> 
	
	


Other libraries and tasks to get over the warnings while running Hadoop commands
---------------------------------------------------------------------------------
Warning that I got is : It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'
http://kuntalganguly.blogspot.in/2014/07/building-native-hadoop-libraries-to-fix.html


Install the following libraries:
sudo apt-get install cmake
sudo apt-get install libc6-dev
sudo apt-get install zlib1g-dev
sudo apt-get install maven
wget https://issues.apache.org/jira/secure/attachment/12570212/HADOOP-9320.patch
patch < HADOOP-9320.patch
	

HDFS Commands
-------------

user commands : fs, dfs , fsck, archive, distcp etc. eg. usage :  hadoop fs 
admin commands : dfsadmin, namenode, datanode

The four main scripts to execute various commands are : hadoop , hdfs , mapred, yarn eg. hadoop <commands>

--> bin/hadoop command [genericOptions] [commandOptions]

 hadoop fs -help <fs commands >>          			      // use this often to know the various possible arguments. hadoop fs -help ls

 hadoop fs -mkdir /hdfsfiles              			      // create the directory in hdfs, this write the data into the 
 hadoop fs -ls /             			  			      // list the files in the specified hadoop folder, specifies the replication factor also as well if it is a file(not a directory)
     eg. hadoop fs -ls -h -R /                  	      // h: formats file sizes in human readable format, -R recursively lists the files.
 hadoop fs -put ./file1 /hdfsfiles		  			      // this puts the local file1 into hdfs filesystem at /hdfsfiles
 hadoop fs -cat /hdfsfiles/file1	      			      // prints the content of the file
 hadoop fs -checksum /hdfsfiles/file1     			      // checksum for the distributed hdfs file
 md5sum file1 							  			      // checksum for the local file
 hadoop fs -appendToFile file2 /hdfsfiles/file1  	      // append local file2 to hdfs file1
 hadoop fs -count /hdfsfiles              			      // count the number of directories, files , size of data in the specified hdfs directory, hadoop fs -count -h -q /
 hadoop fs -df -h                         				  // hadoop's unix df version
 hadoop fs -du /hdfsfiles                 				  // hadops's unix du version
 hadoop fs -copyToLocal /hdfsfiles/file1 ~/file1.local    // copies from hdfssource to local directory
 hadoop fs -get /hdfsfiles/file1 ~/file1.local1           // same as the above command, does not overwrite if the filename exists in the local folder.
 hadoop fs -getmerge /hdfsfiles ~/file.merge			  // merges the files in /hdfsfiles directory can saves the merged file into the local folder.
 hadoop fs -cp /hdfsfiles/file* /hdfsfilesbak             // copies the specified files from on hdfs folder to the other.
 hadoop fs -mv /hdfsfiles/file* /hdfsfilesbak             // moves files, does not overwrite if the files exist.
 hadoop fs -rm /hdfsfiles/file*							  // removes the specified files
 hadoop fs -stat %b /hdfsfiles/file1                      // provides statictics on the specified hdfs folder/files. not much of a use
 hadoop fs -test /hdfsfiles/file1                         // is similar to above stat, but could not see how its work.
 hadoop fsck /hdfsfiles									  // checks the replication details of the blocks.
 hadoop fs setrep 3 /hdfsfiles							  // sets the replication factor for the files under the specified folder.
 hadoop fs -touchz /hdfsfiles/file.touch                  // create a file of zero length at the specified folder.
 hadoop fs -truncate -w 10 /hdfsfiles/file2               // truncates the file to specified length
 hadoop fs -chmod 744 /hdfsfiles/file.touch               // similary -chown and -chgrp commands are available like in linux.
 
 
 hdfs                                                     // lists all the comamnds
 hdfs <command> 										  // throws help for the command under main hdfs command
 hdfs fsck /hdfsfiles									  // same as hadoop fsck command above, note that this has lot of options&parameters.
 hadoop fsck /hdfsfiles -files -list-corruptfileblocks    // list corrupt blocks, etc. explore more params under fsck command, imp
 
 
 
 Other key HDFS parameters
 ----------------------------------------------------------------------------------------------------

 fs.trash.interval , this property to be configured in core-site.xml
 -----------------
 hadoop fs -ls , lists the .Trash directory.
 
 hadoop fs -rm /hdfsfiles/file1                           // delete the hdfs file 
 hadoop fs -cat .Trash/Current/hdfsfiles/file1            // this the folder into which the deleted file goes after delete, and can be recovered by copying back
 hadoop fs -expunge                                       // delete the data in .Trash folder
 
 
 (this is not yet clear, read further)dfs.namenode.acls.enabled=true
 -------------------------------
 hadoop fs -getfacl /hdfsfiles/file1
           -setfacl
		   
		   -getfattr
		   -setfattr
		   
		   
		   

Running the example in examples folder
--------------------------------------
hadoop jar hadoop-mapreduce-examples-2.7.3.jar org.apache.hadoop.examples.WordCount 123		   

Running HadoopTest in Eclipse:
------------------------------
hadoop jar HadoopTest.jar com.company.WordCount /data/read /data/wordcount
