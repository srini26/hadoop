
Hbase Installation
------------------

Download the HBASE version 1.2.6

Hbase documentation reference : http://hbase.apache.org/book.html

Extract and point $HBASE_HOME to the extracted folder.
Add the export HBASE_HOME=/home/user/bigdata/hbase-1.2.6 at end of ~/.bashrc file
Add $HBASE_HOME/bin to the PATH variable in ~/.bashrc file
source ~/.bashrc // run the command 


HBASE standalone configuration:
-------------------------------
hbase-site.xml (in $HBASE_HOME/conf dir) :

<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///home/user/bigdata/hbase-1.2.6/data</value>
  </property>
</configuration>

$ jps  // you would see only one HMaster running, which runs the HMaster, Region server and Zookeeper in single JVM process
$ start-hbase.sh // to start hbase
$ stop-hbase.sh // to stop hbase


HBASE pseudo distributed mode:
------------------------------
hbase-site.xml (in $HBASE_HOME/conf dir) :

<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///home/user/bigdata/hbase-1.2.6/data</value>
  </property>
   <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
 <property>
    <name>hbase.zookeeper.quorum</name>
    <value>ubuntu</value>
  </property>
 <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/home/user/bigdata/hbase-1.2.6/zkdata</value>
  </property>
 <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
  </property>
</configuration>

$ jps  // you would see only 3 processes running: HQuorumPeer, HMaster, HRegionServer.
$ start-hbase.sh // to start hbase
$ stop-hbase.sh // to stop hbase


hbase shell >
help 													// lists the hbase commands
list													// lists the tables in hbase
>incr 'customer','jsmith','addr:counter',1   			// addr:counter should not exist while initializing counter
>incr 'customer','jsmith','addr:counter',10  			// subsequent invocations increaments the counter by the specified value
>delete 'customer','jsmith','addr:age'				  	// deleting the column itself
>delete 'customer','jsmith','addr:age',1506092581130   	// deleting a specific value from a column based on its timestam from history
>delete 'customer2','jsmith'                           	// delete all the columns with specified key
truncate 'customer2'									// disables,truncates and recreates the tables in only one region
>truncate_preserve 'customer2							// disables,truncates and recreates the tables but still maintain the metadata about the table contained in hbase:meta. 

--> Usually HBase handles the splitting of regions automatically by running balancer, we can also launch the same process explicitly.
>balancer												// hbase runs the balancer to distribute the existing data in tables across new regions added for the table, by solving the hot spotting problem
>locate_region 'customer','jsmith'						// shows the regions for the given key
	HOST                                        REGION
	ubuntu:16201                               {ENCODED => 6fc78b416fc089ea2bd5975a864f550d, NAME => 'customer,,1505830224357.6fc78b416fc089ea2bd5975a864f550d.', STARTKEY=> '', ENDKEY => ''}
	
// Below are examples are on splits and regions	
>create 'customer3','personal',SPLITS=>['a','o']
 put 'customer3','aditya','personal:DOB','20090202'    // personal is the column family
 put 'customer3','vikram','personal:DOB','20190202'
 locate_region 'customer3','aditya'
 locate_region 'customer3','vikram'
 split '2d8594cf5d63eda91e91f422cedebeda','s' 			// can split the above region into two
 put 'customer3','vikram2','personal:DOB','20081204'
 locate_region 'customer3','vikram2'
 compact 'customer3'  									// compacts the table into smaller size. can be done on regions or a individual column level
 merge_region 'region1','region2' 						// merges regions into one.
 
 --> Also look at all the various commands available in HBase
 
 -->  hbase rest start -p 8080  // starts the rest server 
      check the url at http://<IP>:8080
	  
	  

 
 
 Hbase Pig Integration
 ----------------------
hbase shell>
create 'people','i'
put 'people','100','i:name','srini'
put 'people','101','i:name','pokuri'

put 'people','100','i:age',30
put 'people','101','i:age',35

put 'people','100','i:place','vijayawada'
put 'people','101','i:place','bangalore'

scan 'people'

// note that the above data gets stored into file defined by hase configuration file, /bigdata/hbase-1.2.6/data/data/default/people/5d7505bcd305b739cc9960df8e49a559/i

mapred historyserver 			// this is the way to start the history server
pig
grunt >
peopledata = load 'hbase://people' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('i:name i:age', '-loadKey true limit 100') as (sno:chararray, name:chararray, age:int);
dump peopledata;
adultsdata = FILTER peopledata BY age>=18;
store adultsdata into 'hbase://adultsdata' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('i:name i:age', '-loadKey true');  // could not run this as the ubuntu machine is hanging and killing the running processes.

// after this exercise no need of history server , kill the process.

HBase Hive Integration
-----------------------
hive>
create database hbase;
create external table people(sno string,name string, age int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties ("hbase.columns.mapping" = ":key,i:name,i:age", "hbase.table.name" = "people");
select * from people;

